{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microsoft Malware Prediction Competition\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import json\n",
    "from multiprocessing import cpu_count\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from textwrap import wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feather\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from prettytable import PrettyTable\n",
    "from pandas_summary import DataFrameSummary\n",
    "from IPython.display import display\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.externals.joblib import Parallel, delayed, parallel_backend\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_pandas import CategoricalImputer\n",
    "try:\n",
    "    from sklearn.impute import SimpleImputer\n",
    "except ImportError:\n",
    "    # scikit-learn <= 0.19\n",
    "    from sklearn.preprocessing import Imputer as SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basedir import TRAIN, TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "num_workers = cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualStyle:\n",
    "    def __init__(self, config, default=None):\n",
    "        if default is None:\n",
    "            default = plt.rcParams\n",
    "        self.default = default.copy()\n",
    "        self.config = config\n",
    "        \n",
    "    def replace(self):\n",
    "        plt.rcParams = self.config\n",
    "    \n",
    "    def override(self):\n",
    "        plt.rcParams.update(self.config)\n",
    "        \n",
    "    def restore(self):\n",
    "        plt.rcParams = self.default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotebookStyle(VisualStyle):\n",
    "    def __init__(self):\n",
    "        super().__init__({\n",
    "            'axes.titlesize': 20,\n",
    "            'axes.labelsize': 18,\n",
    "            'xtick.labelsize': 14,\n",
    "            'ytick.labelsize': 14,\n",
    "            'font.size': 16\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = NotebookStyle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style.override()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_load(train_path):\n",
    "    temp_dir = train_path.parent/'tmp'\n",
    "    temp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    temp_data = temp_dir/'data.feather'\n",
    "    if temp_data.exists():\n",
    "        print(f'Loading previously saved training data: {temp_data}')\n",
    "        data = feather.read_dataframe(temp_data)\n",
    "    else:\n",
    "        print(f'Reading the CSV file with training data: {train_path}')\n",
    "        data = pd.read_csv(train_path, low_memory=False)\n",
    "        print(f'Saving data frame into feather file...')\n",
    "        data.to_feather(temp_data)\n",
    "            \n",
    "    temp_summary = temp_dir/'summary.pickle'\n",
    "    if temp_summary.exists():\n",
    "        print(f'Loading previously saved summary: {temp_summary}')\n",
    "        state = pickle.load(temp_summary.open('rb'))\n",
    "        summary = DataFrameSummary(pd.DataFrame())\n",
    "        summary.__dict__.update(state)\n",
    "    else:\n",
    "        print('Generating summary statistics')\n",
    "        summary = DataFrameSummary(data)\n",
    "        print('Saving summary into pickle file...')\n",
    "        with temp_summary.open('wb') as file:\n",
    "            state = {'length': summary.length, \n",
    "                     'columns_stats': summary.columns_stats,\n",
    "                     'corr': summary.corr}\n",
    "            pickle.dump(state, file)\n",
    "        \n",
    "    return data, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, summary = create_or_load(TRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-Level Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_all_columns(df):\n",
    "    with pd.option_context('display.max_columns', None):\n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_columns(summary.columns_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary matrix could give us useful insights about the dataset. Let's take a close look at the data structure before proceeding with a more thorough analysis. \n",
    "\n",
    "The very first feature of the dataset, `MachineIdentifier`, is a unique key so it can be excluded from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = summary.columns_stats.drop(columns='MachineIdentifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, the summary matrix tell us how many missing values has each dataset column. If the most of column values are missing, it probably will not be a good predictor, or at least, will require a special treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_if_needed(ax, figsize=(4, 4)):\n",
    "    \"\"\"Creates an axis but only if provided `ax` argument is None.\"\"\"\n",
    "    if ax is None:\n",
    "        f, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    else:\n",
    "        f = ax.figure()\n",
    "    return f, ax    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_missing_percentage_hist(stats, ax=None, figsize=(10, 6)):\n",
    "    missing = stats.loc['missing_perc'].map(lambda x: float(x.strip('%'))/100)\n",
    "    non_zero = missing[missing > 0]\n",
    "    ticks = np.linspace(0, 1, 11)\n",
    "    tick_labels = [f'{t:2.0%}' for t in ticks]\n",
    "    _, ax = create_if_needed(ax, figsize)\n",
    "    ax.hist(non_zero, bins=20, align='mid')\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels(tick_labels)\n",
    "    ax.set_title(f'Distribution of Missing Values Percentage Among Features\\n'\n",
    "                 f'({len(non_zero):d} of {len(missing):d} columns have missing values)')\n",
    "    ax.set_xlabel(f'Proportion of missing values\\n'\n",
    "                  f'{sum(non_zero <= 0.5)} features <= 50% < {sum(non_zero > 0.5)} features')\n",
    "    ax.set_ylabel('# of features')\n",
    "    ax.axvline(0.5, color='red', lw=2)\n",
    "    ax.grid(True)\n",
    "    return ax, missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax, missing = plot_missing_percentage_hist(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the historgram tells us that exactly the half of the dataset features contains missing values, but only `7` features have more that `50%` of missing values. Though there are also a couple of columns with more then `95%` of missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing.sort_values(ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 3 columns contains extremely small fraction of not None values and should be treated accordintly. One can drop them all together from the analysis, or create an additional model that could deal with the rare data and support the decision making process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also it makes sense to check how many unique values per column we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_unique_values_hist(stats, log_scale=True, ax=None, figsize=(10, 14)):\n",
    "    uniques = stats.loc['uniques'].sort_values(ascending=False)\n",
    "    uniq_values = uniques.values.astype(int)\n",
    "    uniq_names = list(uniques.keys())\n",
    "    if log_scale:\n",
    "        uniq_values = np.log(uniq_values)\n",
    "    f, ax = create_if_needed(ax, figsize)\n",
    "    x = [i for i, _ in enumerate(uniq_names)]\n",
    "    ax.barh(x, uniq_values)\n",
    "    ax.set_yticks(x)\n",
    "    ax.set_yticklabels(uniq_names)    \n",
    "    ax.margins(y=0)\n",
    "    ax.set_title('Number of Unique Values per Feature Column')\n",
    "    ax.set_xlabel('log(# of unique values)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_unique_values_hist(stats, figsize=(8, 18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The historgram shows us that many of `Census` columns have high cardinality, and the most unique ones are `CityIdentifier`, `Census_OEMModelIdentifier`, and `Census_SystemVolumeTotalCapacity`. \n",
    "\n",
    "Let's check the absolute values for the columns with high number of unique values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.loc['uniques'].sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It could be a good idea to reduce the cardinality of these features grouping them into more generic features, or use some embedding technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to start a more deep analysis of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, summary = create_or_load(TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_column = 'MachineIdentifier'\n",
    "\n",
    "target_column = 'HasDetections'\n",
    "\n",
    "num_columns = [\n",
    "    'Census_TotalPhysicalRAM',\n",
    "    'Census_InternalPrimaryDiagonalDisplaySizeInInches',\n",
    "    'Census_InternalBatteryNumberOfCharges']\n",
    "\n",
    "cat_columns = [col for col in data.columns if col not in num_columns]\n",
    "cat_columns.remove(target_column)\n",
    "cat_columns.remove(id_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(dataset, target_column, test_size=0.1, seed=seed, subset=None):\n",
    "    if subset is not None:\n",
    "        dataset = dataset.sample(subset)\n",
    "    n = len(dataset)\n",
    "    train, valid = train_test_split(\n",
    "        np.arange(n), test_size=test_size, random_state=seed, stratify=dataset[target_column])\n",
    "    X, y = dataset.drop(columns=target_column), dataset[target_column]\n",
    "    print(f'Train: {len(train)} / Valid: {len(valid)}')\n",
    "    return X.iloc[train], X.iloc[valid], y.iloc[train], y.iloc[valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier = data[id_column]\n",
    "data.drop(columns=id_column, inplace=True)\n",
    "X_train, X_test, y_train, y_test = create_train_test(data, target_column)\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_columns(X_train.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Detections: {sum(y_train)} yes, {sum(1 - y_train)} no ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropNonInformative(TransformerMixin):\n",
    "    \n",
    "    def __init__(self, max_missing=0.8, max_uniq_ratio=0.7, max_value_freq=0.95, copy=False):\n",
    "        self.max_missing = max_missing\n",
    "        self.max_uniq_ratio = max_uniq_ratio\n",
    "        self.max_value_freq = max_value_freq\n",
    "        self.copy = copy\n",
    "        \n",
    "    def fit(self, X, summary=None, **fit_kwargs):\n",
    "        if summary is None:\n",
    "            summary = DataFrameSummary(X)\n",
    "            \n",
    "        drop = {}\n",
    "        stats = summary.columns_stats.T\n",
    "        for column in X.columns:\n",
    "            if self.max_missing is not None:\n",
    "                pct = float(stats.loc[column]['missing_perc'].strip('%'))/100\n",
    "                if pct >= self.max_missing:\n",
    "                    drop[column] = 'many missing values'\n",
    "                    \n",
    "            if self.max_uniq_ratio is not None:\n",
    "                ratio = stats.loc[column]['uniques']/stats.loc[column]['counts']\n",
    "                if ratio >= self.max_uniq_ratio:\n",
    "                    drop[column] = 'too many unique values'\n",
    "                    \n",
    "            if self.max_value_freq is not None:\n",
    "                freqs = X[column].dropna().value_counts()/stats.loc[column]['counts']\n",
    "                for val, freq in freqs.items():\n",
    "                    if freq > self.max_value_freq:\n",
    "                        drop[column] = f'too many rows with same value: {val}'\n",
    "                        break\n",
    "        \n",
    "        self.drop_reason_ = drop\n",
    "        self.drop_columns_ = list(drop.keys())\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        check_columns(X, self.drop_columns_)\n",
    "        if self.copy:\n",
    "            X = X.copy()\n",
    "        X.drop(columns=self.drop_columns_, inplace=True)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplaceNoneStrings(TransformerMixin):\n",
    "    \"\"\"Replaces various ways to represent None value with np.nan.\n",
    "    \n",
    "    Parameters:\n",
    "        values: A set of values that represent None and should be replaces with np.nan\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, na_strings=('None', '', 'n/a', 'N/A', 'NA'), replacement=np.nan, copy=False):\n",
    "        self.na_strings = na_strings\n",
    "        self.replacement = replacement\n",
    "        self.copy = copy\n",
    "        \n",
    "    def fit(self, X, **fit_kwargs):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        if self.copy:\n",
    "            X = X.copy()\n",
    "        replace = {value: self.replacement for value in self.na_strings}\n",
    "        return X.replace(to_replace=replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsType(TransformerMixin):\n",
    "    \"\"\"Casts data frame to a specific type.\"\"\"\n",
    "    def __init__(self, dtype):\n",
    "        self.dtype = dtype\n",
    "        \n",
    "    def fit(self, X, **fit_kwargs):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X.astype(self.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectColumns(TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "    \n",
    "    def fit(self, X, **fit_kwargs):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_columns(df, columns):\n",
    "    if columns is None:\n",
    "        raise ValueError('columns list is None')\n",
    "    for col in columns:\n",
    "        if col not in df:\n",
    "            raise ValueError(f'column is not present in the dataframe: {col}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = DataFrameSummary(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipe = Pipeline([\n",
    "    ('select', SelectColumns(num_columns)),\n",
    "    ('cast', AsType(np.float32)),\n",
    "    ('drop', DropNonInformative()),\n",
    "    ('imputer', SimpleImputer(strategy='mean', copy=False))])\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    ('select', SelectColumns(cat_columns)),\n",
    "    ('cast', AsType(str)),\n",
    "    ('replace', ReplaceNoneStrings()),\n",
    "    ('drop', DropNonInformative()),\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent', copy=False)),\n",
    "    ('encoder', OrdinalEncoder())])\n",
    "\n",
    "pipe = FeatureUnion([('num', num_pipe), ('cat', cat_pipe)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipEmpty(TransformerMixin):\n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "    def fit(self, X, **fit_kwargs):\n",
    "        if X is None or X.shape[0] == 0 or X.shape[1] == 0:\n",
    "            breakpoint()\n",
    "            return self.estimator\n",
    "        return self.estimator.fit(X, **fit_kwargs)\n",
    "    def transform(self, X, y=None):\n",
    "        if X is None or X.shape[0] == 0 or X.shape[1] == 0:\n",
    "            return X\n",
    "        return self.estimator.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(name, trn, val, categorical, summary):\n",
    "    stats = summary.columns_stats.T\n",
    "    if categorical:\n",
    "        pipe = [\n",
    "            ('cast', AsType(str)),\n",
    "            ('replace', ReplaceNoneStrings()),\n",
    "            ('drop', DropNonInformative()),\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent', copy=False)),\n",
    "            ('encoder', OrdinalEncoder())]\n",
    "    else:\n",
    "        pipe = [\n",
    "            ('cast', AsType(np.float32)),\n",
    "            ('drop', DropNonInformative()),\n",
    "            ('imputer', SimpleImputer(strategy='mean', copy=False))]\n",
    "        \n",
    "    enc_trn = trn\n",
    "    for name, estimator in pipe:\n",
    "        enc_trn = estimator.fit_transform(enc_trn, summary=summary)\n",
    "        if enc_trn is None or enc_trn.shape[0] == 0 or enc_trn.shape[1] == 0:\n",
    "            return {}\n",
    "    \n",
    "    enc_val = val\n",
    "    for name, estimator in pipe:\n",
    "        enc_val = estimator.transform(enc_val)\n",
    "        \n",
    "#     enc_trn = pipe.fit_transform(trn, drop__summary=summary)\n",
    "#     enc_val = pipe.transform(val)\n",
    "    return {'pipe': pipe, 'encoded': (enc_trn, enc_val)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'Census_ProcessorClass'\n",
    "worker(col, X_train[[col]], X_test[[col]], True, summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Parallel(n_jobs=num_workers) as parallel:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trn_enc = pipe.fit_transform(X_train, drop__summary=summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_enc = pipe.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tiny_trn = y_tiny_trn.values.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipe.named_steps['encoder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tiny_val = pipe.transform(X_tiny_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tiny_val = y_tiny_val.values.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = lgb.LGBMClassifier(n_estimators=10)\n",
    "gbm.fit(X, y, eval_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
