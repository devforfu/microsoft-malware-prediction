{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Engineering\n",
    "\n",
    "This notebook contains various approaches to encode and engineer dataset featuers. In general, the dataset encoding depends on the model applied. Therefore it is worth to try various data preprocessing approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_summary import DataFrameSummary\n",
    "\n",
    "from basedir import TRAIN, TEST, DATA\n",
    "from info import efficient_types, features, cat_features, num_features, target_feature, id_feature\n",
    "from utils import show_all_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN, usecols=efficient_types.keys(), dtype=efficient_types)\n",
    "identifier = train_df[id_feature].copy()\n",
    "target = train_df[target_feature].copy()\n",
    "train_df.drop(columns=[id_feature, target_feature], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_columns(train_df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = DataFrameSummary(train_df)\n",
    "stats = summary.columns_stats\n",
    "del summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_columns(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_perc = stats.loc['missing_perc'].map(lambda s: float(s.strip('%'))/100)\n",
    "raw_train_df = train_df.copy()\n",
    "train_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = len(raw_train_df)\n",
    "percent = int(0.01*n)\n",
    "total = len(features)\n",
    "unk = 'unknown'\n",
    "\n",
    "encoders = {}\n",
    "\n",
    "for i, col in enumerate(features, 1):\n",
    "    values = raw_train_df[col].copy()\n",
    "    encoders[col] = {}\n",
    "\n",
    "    if col in cat_features:\n",
    "        print('[%02d/%02d] Encoding: %s' % (i, total, col))\n",
    "        values = values.astype(str).replace('nan', unk)\n",
    "        if missing_perc[col] == 0:\n",
    "            unknown = np.random.choice(n, size=percent, replace=False)\n",
    "            encoders[col]['replaced'] = values[unknown]\n",
    "            values[unknown] = unk\n",
    "        uniq = [x for x in values.unique() if x != unk]\n",
    "        labels = {x: i for i, x in enumerate(uniq)}\n",
    "        labels[unk] = -1\n",
    "        values = values.map(labels).astype(int)\n",
    "        encoders[col]['labels'] = labels\n",
    "        \n",
    "    elif col in num_features:\n",
    "        print('[%02d/%02d] Imputing: %s' % (i, total, col))\n",
    "        impute = values.astype(np.float64).mean()  # otherwise could return NaN if float16 type\n",
    "        values[pd.isnull(values)] = impute\n",
    "        encoders[col]['impute'] = impute\n",
    "        values = values.astype(np.float32)\n",
    "    \n",
    "    train_df[col] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_columns(train_df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pd.isnull(train_df).sum().sum() == 0, 'Still has NaN values after preprocessing!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_types = efficient_types.copy()\n",
    "test_types.pop('HasDetections')\n",
    "raw_test_df = pd.read_csv(TEST, usecols=test_types.keys(), dtype=test_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame()\n",
    "n = len(raw_test_df)\n",
    "total = len(features)\n",
    "\n",
    "for i, col in enumerate(features, 1):\n",
    "    print('[%02d/%02d] Processing feature: %s' % (i, total, col))\n",
    "    values = raw_test_df[col].copy()\n",
    "    encoder = encoders[col]\n",
    "    if 'impute' in encoder:\n",
    "        values = values.astype(np.float32)\n",
    "        values[pd.isnull(values)] = encoder['impute']\n",
    "    if 'labels' in encoder:\n",
    "        labels = encoder['labels'].copy()\n",
    "        uniq = values.unique()\n",
    "        for value in uniq:\n",
    "            if value not in labels:\n",
    "                labels[value] = labels[unk]\n",
    "        values = values.map(labels).astype(int)\n",
    "    test_df[col] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pd.isnull(test_df).sum().sum() == 0, 'Still has NaN values after preprocessing!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_indicies = [i for i, x in enumerate(features) if x in cat_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'identifier': identifier,\n",
    "    'target': target,\n",
    "    'train_df': train_df,\n",
    "    'test_df': test_df,\n",
    "    'encoders': encoders,\n",
    "    'cat_indicies': cat_indicies,\n",
    "    'stats': stats}\n",
    "\n",
    "filename = DATA/'dataset.pickle'\n",
    "with filename.open('wb') as file:\n",
    "    pickle.dump(dataset, file, protocol=4)\n",
    "\n",
    "print(f'Data and meta information saved into {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
