import argparse
from hashlib import md5

import pandas as pd

from basedir import DATA
from info import id_feature, target_feature, num_features, cat_features


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '-hdf',
        required=True, dest='filename',
        help='Path to the HDF5 file with data'
    )
    parser.add_argument(
        '-k',
        required=True, dest='key',
        help='HDF5 file key'
    )
    parser.add_argument(
        '-o',
        dest='output', default='out.vw',
        help='Output file name'
    )
    parser.add_argument(
        '-root',
        default=DATA,
        help='Relative path to load the data'
    )
    parser.add_argument(
        '-abs',
        default=False, action='store_true',
        help='Use absolute file path'
    )

    args = parser.parse_args()
    filename, key, output = args.filename, args.key, args.output
    if not args.abs:
        filename = DATA/filename
        output = DATA/output

    data = pd.read_hdf(filename, key=key, chunksize=100000)

    short = {feat: f'f{i}' for i, feat in enumerate(cat_features + num_features)}
    print(f'Saving dataset into file {output}')
    with output.open('w') as file:
        for i, chunk in enumerate(data, 1):
            print(f'Writing chunk #{i:d} of size {len(chunk)}')
            for r in chunk.itertuples():
                r = r._asdict()
                target = r.get(target_feature, 0)
                identity = r[id_feature]
                cats = ' '.join([f'{short[f]}_{r[f]}' for f in cat_features])
                nums = ' '.join([f'{short[f]}:{r[f]}' for f in num_features])
                sample = f"{target} 'index={identity} |cat {cats} |num {nums}\n"
                file.write(sample)
    print('Done!')


if __name__ == '__main__':
    main()
