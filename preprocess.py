import argparse
from multiprocessing import cpu_count
import pickle

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.externals.joblib import Parallel, delayed

from basedir import TRAIN
from utils import create_or_load


seed = 1
np.random.seed(seed)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('-j', dest='jobs', default=1, type=int, help='num of jobs')
    args = parser.parse_args()
    n_jobs = args.jobs
    data, summary = create_or_load(TRAIN)

    id_column = 'MachineIdentifier'
    target_column = 'HasDetections'
    num_columns = [
        'Census_TotalPhysicalRAM',
        'Census_InternalPrimaryDiagonalDisplaySizeInInches',
        'Census_InternalBatteryNumberOfCharges']
    cat_columns = [col for col in data.columns if col not in num_columns]
    cat_columns.remove(target_column)
    cat_columns.remove(id_column)

    identifier = data[id_column]
    stats = summary.columns_stats.T
    data.drop(columns=id_column, inplace=True)
    subsets = create_train_test(data, target_column)
    results = prepare_parallel(subsets, stats, num_columns, cat_columns, n_jobs)

    print('Saving output on disk...')
    info, dropped = results.pop('info'), results.pop('dropped')
    results['id'] = identifier.values.todict()
    np.savez('data.npz', **results)
    with open('meta.pickle', 'wb') as file:
        pickle.dump({'info': info, 'dropped': dropped}, file)
    print('Done!')


def create_train_test(dataset, target_column, test_size=0.1, seed=seed):
    n = len(dataset)
    train, valid = train_test_split(
        np.arange(n), test_size=test_size, random_state=seed,
        stratify=dataset[target_column])
    X, y = dataset.drop(columns=target_column), dataset[target_column]
    print(f'Train: {len(train)} / Valid: {len(valid)}')
    return X.iloc[train], X.iloc[valid], y.iloc[train], y.iloc[valid]


def prepare_parallel(data: tuple, stats: pd.DataFrame, num_cols: list, cat_cols: list, n_jobs=None):
    n_jobs = n_jobs or cpu_count()
    x_trn, x_val, y_trn, y_val = data
    cols = x_trn.columns.tolist()

    batches = []
    for i, c in enumerate(cols):
        if c not in num_cols and c not in cat_cols:
            continue
        cat = c in cat_cols
        batches.append({
            'name': c, 'order': i, 'trn_col': x_trn[c], 'val_col': x_val[c],
            'stats': stats.loc[c], 'categorical': cat})

    print('Running columns preprocessing...')
    with Parallel(n_jobs=n_jobs) as parallel:
        results = parallel(delayed(prepare_single)(**b) for b in batches)

    print('Concatenating results...')
    x_trn_enc, x_val_enc = [], []
    info = {}
    dropped = []
    for name, result in results:
        if 'dropped' in result:
            print(f'Warning! The column \'{name}\' was dropped: {result["dropped"]}')
            dropped.append(name)
            continue
        x_trn_enc.append(result.pop('x_trn'))
        x_val_enc.append(result.pop('x_val'))
        info[name] = result

    x_trn = np.hstack(x_trn_enc)
    x_val = np.hstack(x_val_enc)
    y_trn = y_trn.astype(np.float32)
    y_val = y_val.astype(np.float32)

    return {'x_trn': x_trn, 'x_val': x_val,
            'y_trn': y_trn, 'y_val': y_val,
            'info': info, 'dropped': dropped}


def prepare_single(name, order, trn_col, val_col, stats, categorical):
    print(f'Preparing column \'{name}\'')
    ok, message = is_informative(trn_col, stats)

    if not ok:
        print('Skipping as non informative...')
        return name, {'dropped': message}

    result = {'order': order}

    if categorical:
        x = trn_col
        x = x.astype(str)
        x = x.replace({x: np.nan for x in ('None', 'nan', '', 'n/a', 'NA')})
        most_freq, _ = next(x.value_counts().items())
        x[pd.isnull(x)] = most_freq
        labels = {x: i for i, x in enumerate(x.unique())}
        result['impute'] = most_freq
        result['encode'] = labels
        result['x_trn'] = x.map(labels)
        result['cardinality'] = len(x.unique())

        x = val_col
        x = x.astype(str)
        x = x.replace({x: np.nan for x in ('None', 'nan', '', 'n/a', 'NA')})
        x[pd.isnull(x)] = most_freq
        result['x_val'] = x.map(labels)

    else:
        x = trn_col
        x = x.astype(np.float32)
        mean = x.mean()
        x[pd.isnull(x)] = mean
        result['impute'] = mean
        result['x_trn'] = x

        x = val_col
        x = x.astype(np.float32)
        x[pd.isnull(x)] = mean
        result['x_val'] = x

    for key in ('x_trn', 'x_val'):
        result[key] = result[key].values.reshape(-1, 1)

    return name, result


def is_informative(x, stats, max_missing=0.85, max_uniq_ratio=0.7, max_value_freq=0.95):
    if max_missing is not None:
        pct = float(stats['missing_perc'].strip('%'))
        if pct >= max_missing:
            return False, 'too many missing values'

    if max_uniq_ratio:
        ratio = stats['uniques']/stats['counts']
        if ratio >= max_uniq_ratio:
            return False, 'too many unique values'

    if max_value_freq:
        freqs = x.dropna().value_counts()/stats['counts']
        for val, freq in freqs.items():
            if freq > max_value_freq:
                return False, f'too many rows with value \'{val}\''

    return True, None


if __name__ == '__main__':
    main()
