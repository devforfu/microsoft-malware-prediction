{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook is a copy of [this kernel](https://www.kaggle.com/bogorodvo/lightgbm-all-as-category)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import gc\n",
    "from itertools import chain, combinations\n",
    "from pprint import pprint, pformat\n",
    "\n",
    "import feather\n",
    "import hyperopt\n",
    "from hyperopt import hp, Trials\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, ParameterSampler\n",
    "\n",
    "from basedir import TRAIN, TEST, DATA\n",
    "from info import features, efficient_types, target_feature, id_feature\n",
    "from utils import Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "PREPARE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_counts(df, val_col, id_col, label):\n",
    "    return df.groupby(val_col).agg({id_col: 'count'}).reset_index().rename({id_col: label}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_rare_examples_with_nulls(df, cnt, col):\n",
    "    merged = pd.merge(df[[col]], cnt[[col, f'{col}Copy']], on=col, how='left')[f'{col}Copy']\n",
    "    return merged.replace(np.nan, 0).astype(np.int).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previously prepared data\n"
     ]
    }
   ],
   "source": [
    "if PREPARE:\n",
    "    print('Reading the data from CSV files...')\n",
    "    trn_df = pd.read_csv(TRAIN, usecols=efficient_types.keys(), dtype=efficient_types)\n",
    "    trn_df[id_feature] = trn_df.index.astype('uint32')\n",
    "    test_types = efficient_types.copy()\n",
    "    del test_types[target_feature]\n",
    "    tst_df = pd.read_csv(TEST, usecols=test_types.keys(), dtype=test_types)\n",
    "    tst_df[id_feature] = tst_df.index.astype('uint32')\n",
    "\n",
    "    print('Preparing data for training')\n",
    "    total = len(features)\n",
    "    for i, feature in enumerate(features):\n",
    "        print('[%02d/%02d] %s' % (i+1, total, feature))\n",
    "        train, test = [df[feature].astype(str) for df in (trn_df, tst_df)]\n",
    "        uniqs = train.unique().tolist() + test.unique().tolist()\n",
    "        encoder = LabelEncoder().fit(uniqs)\n",
    "        trn_df[feature] = encoder.transform(train) + 1\n",
    "        tst_df[feature] = encoder.transform(test) + 1\n",
    "        trn_cnt = compute_counts(trn_df, feature, id_feature, 'Train')\n",
    "        tst_cnt = compute_counts(tst_df, feature, id_feature, 'Test')\n",
    "        cnt = pd.merge(trn_cnt, tst_cnt, on=feature, how='outer').replace(np.nan, 0)\n",
    "        cnt = cnt[cnt['Train'] > 1000].reset_index(drop=True)\n",
    "        cnt['Total'] = cnt['Train'] + cnt['Test']\n",
    "        cnt['TrainRatio'] = cnt['Train'] / cnt['Total']\n",
    "        cnt = cnt[(cnt.TrainRatio > 0.2) & (cnt.TrainRatio < 0.8)]\n",
    "        cnt[f'{feature}Copy'] = cnt[feature]\n",
    "        trn_df[feature] = replace_rare_examples_with_nulls(trn_df, cnt, feature)\n",
    "        tst_df[feature] = replace_rare_examples_with_nulls(tst_df, cnt, feature)\n",
    "\n",
    "    trn_df.to_feather(DATA/'kernel_trn.feather')\n",
    "    tst_df.to_feather(DATA/'kernel_tst.feather')\n",
    "    \n",
    "else:\n",
    "    print('Loading previously prepared data')\n",
    "    trn_df = feather.read_dataframe(DATA/'kernel_trn.feather')\n",
    "    tst_df = feather.read_dataframe(DATA/'kernel_tst.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_target = trn_df[target_feature].copy()\n",
    "del trn_df[target_feature], trn_df[id_feature], tst_df[id_feature]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'colsample_bytree': 0.28,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': -1,\n",
    "    'num_leaves': 2**12 - 1,\n",
    "    'n_estimators': 30000,\n",
    "    'objective': 'binary',\n",
    "    'random_state': seed,\n",
    "    'n_jobs': -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_idx, val_idx = train_test_split(trn_df.index, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trn = trn_df[trn_df.index.isin(trn_idx)]\n",
    "x_val = trn_df[trn_df.index.isin(val_idx)]\n",
    "y_trn = trn_target[trn_target.index.isin(trn_idx)]\n",
    "y_val = trn_target[trn_target.index.isin(val_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_trn, y_trn,\n",
    "          eval_metric='auc',\n",
    "          eval_set=[(x_val, y_val)],\n",
    "          verbose=100,\n",
    "          early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv(DATA/'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict_proba(tst_df)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit['HasDetections'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('kernel_clone.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle c submit -c microsoft-malware-prediction -f \"kernel_clone.csv\" -m \"Kernel clone (no folds)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_single_split(model_cls, params, train, target, test, seed=seed, **fit_params):\n",
    "    trn_idx, val_idx = train_test_split(train.index, test_size=0.2, random_state=seed)\n",
    "    x_trn = train[train.index.isin(trn_idx)]\n",
    "    x_val = train[train.index.isin(val_idx)]\n",
    "    y_trn = target[target.index.isin(trn_idx)]\n",
    "    y_val = target[target.index.isin(val_idx)]\n",
    "    fit_params['eval_set'] = [(x_val, y_val)]\n",
    "    model = model_cls(**params)\n",
    "    model.fit(x_trn, y_trn, **fit_params)\n",
    "    preds = model.predict_proba(test)[:, 1]\n",
    "    return model, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_kfold(model_cls, params, train, target, test, n_splits=5, seed=seed, **fit_params):\n",
    "    split = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    test_result = np.zeros(len(train))\n",
    "    n = split.get_n_splits(train.index, target)\n",
    "    models = []\n",
    "    \n",
    "    for i, (trn_idx, val_idx) in enumerate(split.split(train.index, target), 1):\n",
    "        print('Running split %02d/%02d' % (i, n))\n",
    "        x_trn = train[train.index.isin(trn_idx)]\n",
    "        x_val = train[train.index.isin(val_idx)]\n",
    "        y_trn = target[target.index.isin(trn_idx)]\n",
    "        y_val = target[target.index.isin(val_idx)]\n",
    "        fit_params_curr = fit_params.copy()\n",
    "        fit_params_curr['eval_set'] = [(x_val, y_val)]\n",
    "\n",
    "        model = model_cls(**params)\n",
    "        model.fit(x_trn, y_trn, **fit_params)\n",
    "        models.append(model)\n",
    "        test_result += model.predict_proba(test)[:, 1]\n",
    "\n",
    "        del x_trn, x_val, y_trn, y_val\n",
    "        gc.collect()\n",
    "        \n",
    "    test_result /= n_splits\n",
    "        \n",
    "    return models, test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aslist(x):\n",
    "    if hasattr(x, '__len__'):\n",
    "        return list(x)\n",
    "    return [x]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(data, keys, relative=True):\n",
    "    cnt = data.groupby(keys).size()\n",
    "    if relative:\n",
    "        cnt /= len(data)\n",
    "    return {tuple(aslist(k)): v for k, v in cnt.to_dict().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit(preds, sample_file, out_file='submit.csv'):\n",
    "    submit = pd.read_csv(sample_file)\n",
    "    submit['HasDetections'] = preds\n",
    "    submit.to_csv(out_file, index=None)\n",
    "    return out_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features =[ \n",
    "    'AvSigVersion',\n",
    "    'Census_OSVersion',\n",
    "    'EngineVersion',\n",
    "    'AVProductStatesIdentifier',\n",
    "    'Census_OSBuildRevision',\n",
    "    'AppVersion',\n",
    "    'Census_OSInstallTypeName',\n",
    "    'OsBuildLab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_groups = [list(g) for g in chain(*[combinations(features, i) for i in range(1, 2)])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_mean = trn_target.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, rounds = len(trn_df), 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_df[target_feature] = target_trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in feature_groups:\n",
    "    name = f'{\"_\".join(g)}_MeanTarget'\n",
    "    print(f'Mean encoding feature:', name)\n",
    "    print('\\tround:', end=' ')\n",
    "    trn_df[name] = 0\n",
    "    for i in range(rounds):\n",
    "        print(f'{i}..', end=' ')\n",
    "        perm = trn_df.copy()[trn_df.index.isin(np.random.choice(n, size=n, replace=False))]\n",
    "        cumsum = perm.groupby(g)[target_feature].cumsum() - perm[target_feature]\n",
    "        cumcnt = perm.groupby(g)[target_feature].cumcount()\n",
    "        trn_df[name] += (cumsum/cumcnt).fillna(global_mean)\n",
    "    print()\n",
    "    trn_df[name] /= rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in feature_groups:\n",
    "    name = f'{\"_\".join(g)}_MeanTarget'\n",
    "    print('Generating test set values for the mean feature:', name)\n",
    "    train_groups = trn_df.groupby(g).groups\n",
    "    train_keys = list(train_groups)\n",
    "    test_keys = list(tst_df.groupby(g).groups.keys())\n",
    "    for key in test_keys:\n",
    "        filters = [tst_df[feat] == value for feat, value in zip(g, aslist(key))]\n",
    "        subset = reduce(lambda a, b: a & b, filters)\n",
    "        if key not in train_keys or len(train_groups[key]) == 0:\n",
    "            tst_df.loc[subset, name] = global_mean\n",
    "        else:\n",
    "            sample = trn_df[name][train_groups[key]].sample(subset.sum(), replace=True)\n",
    "            tst_df.loc[subset, name] = sample.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del trn_df[target_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_df.to_feather(DATA/'kernel_trn+mean.feather')\n",
    "tst_df.to_feather(DATA/'kernel_tst+mean.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_df = feather.read_dataframe(DATA/'kernel_trn+mean.feather')\n",
    "tst_df = feather.read_dataframe(DATA/'kernel_tst+mean.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'colsample_bytree': 0.28,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': -1,\n",
    "    'num_leaves': 2**12 - 1,\n",
    "    'n_estimators': 30000,\n",
    "    'objective': 'binary',\n",
    "    'random_state': seed,\n",
    "    'n_jobs': -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's auc: 0.730158\tvalid_0's binary_logloss: 0.606956\n",
      "[200]\tvalid_0's auc: 0.733116\tvalid_0's binary_logloss: 0.601929\n",
      "[300]\tvalid_0's auc: 0.733271\tvalid_0's binary_logloss: 0.601162\n",
      "Early stopping, best iteration is:\n",
      "[245]\tvalid_0's auc: 0.733491\tvalid_0's binary_logloss: 0.601171\n"
     ]
    }
   ],
   "source": [
    "_, preds = fit_single_split(\n",
    "    lgb.LGBMClassifier, params, trn_df, trn_target, tst_df, \n",
    "    eval_metric='auc',\n",
    "    verbose=100,\n",
    "    early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 391M/391M [00:35<00:00, 11.5MB/s]\n",
      "Successfully submitted to Microsoft Malware Prediction"
     ]
    }
   ],
   "source": [
    "submit = pd.read_csv(DATA/'sample_submission.csv')\n",
    "submit['HasDetections'] = preds\n",
    "submit.to_csv('kernel_clone.csv', index=None)\n",
    "!kaggle c submit -c microsoft-malware-prediction -f \"kernel_clone.csv\" -m \"Kernel clone (+ mean enc)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_df = feather.read_dataframe(DATA/'kernel_trn.feather')\n",
    "tst_df = feather.read_dataframe(DATA/'kernel_tst.feather')\n",
    "trn_target = trn_df[target_feature].copy()\n",
    "del trn_df[target_feature], trn_df[id_feature], tst_df[id_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = len(trn_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features =[ \n",
    "    'AvSigVersion',\n",
    "    'Census_OSVersion',\n",
    "    'EngineVersion',\n",
    "    'AVProductStatesIdentifier',\n",
    "    'Census_OSBuildRevision',\n",
    "    'AppVersion',\n",
    "    'Census_OSInstallTypeName',\n",
    "    'OsBuildLab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([trn_df, tst_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating counting feature: AvSigVersion_Freq\n",
      "Creating counting feature: Census_OSVersion_Freq\n",
      "Creating counting feature: EngineVersion_Freq\n",
      "Creating counting feature: AVProductStatesIdentifier_Freq\n",
      "Creating counting feature: Census_OSBuildRevision_Freq\n",
      "Creating counting feature: AppVersion_Freq\n",
      "Creating counting feature: Census_OSInstallTypeName_Freq\n",
      "Creating counting feature: OsBuildLab_Freq\n"
     ]
    }
   ],
   "source": [
    "for feat in features:\n",
    "    name = f'{feat}_Freq'\n",
    "    print('Creating counting feature:', name)\n",
    "    cnt = data.groupby(feat).size().to_dict()\n",
    "    trn_df[name] = trn_df[feat].map(cnt)\n",
    "    tst_df[name] = tst_df[feat].map(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_groups = [list(g) for g in chain(*[combinations(features, i) for i in range(1, 3)])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new feature: AvSigVersion_Freq\n",
      "Creating new feature: Census_OSVersion_Freq\n",
      "Creating new feature: EngineVersion_Freq\n",
      "Creating new feature: AVProductStatesIdentifier_Freq\n",
      "Creating new feature: Census_OSBuildRevision_Freq\n",
      "Creating new feature: AppVersion_Freq\n",
      "Creating new feature: Census_OSInstallTypeName_Freq\n",
      "Creating new feature: OsBuildLab_Freq\n",
      "Creating new feature: AvSigVersion_Census_OSVersion_Freq\n",
      "Creating new feature: AvSigVersion_EngineVersion_Freq\n",
      "Creating new feature: AvSigVersion_AVProductStatesIdentifier_Freq\n",
      "Creating new feature: AvSigVersion_Census_OSBuildRevision_Freq\n",
      "Creating new feature: AvSigVersion_AppVersion_Freq\n",
      "Creating new feature: AvSigVersion_Census_OSInstallTypeName_Freq\n",
      "Creating new feature: AvSigVersion_OsBuildLab_Freq\n",
      "Creating new feature: Census_OSVersion_EngineVersion_Freq\n",
      "Creating new feature: Census_OSVersion_AVProductStatesIdentifier_Freq\n",
      "Creating new feature: Census_OSVersion_Census_OSBuildRevision_Freq\n",
      "Creating new feature: Census_OSVersion_AppVersion_Freq\n",
      "Creating new feature: Census_OSVersion_Census_OSInstallTypeName_Freq\n",
      "Creating new feature: Census_OSVersion_OsBuildLab_Freq\n",
      "Creating new feature: EngineVersion_AVProductStatesIdentifier_Freq\n",
      "Creating new feature: EngineVersion_Census_OSBuildRevision_Freq\n",
      "Creating new feature: EngineVersion_AppVersion_Freq\n",
      "Creating new feature: EngineVersion_Census_OSInstallTypeName_Freq\n",
      "Creating new feature: EngineVersion_OsBuildLab_Freq\n",
      "Creating new feature: AVProductStatesIdentifier_Census_OSBuildRevision_Freq\n",
      "Creating new feature: AVProductStatesIdentifier_AppVersion_Freq\n",
      "Creating new feature: AVProductStatesIdentifier_Census_OSInstallTypeName_Freq\n",
      "Creating new feature: AVProductStatesIdentifier_OsBuildLab_Freq\n",
      "Creating new feature: Census_OSBuildRevision_AppVersion_Freq\n",
      "Creating new feature: Census_OSBuildRevision_Census_OSInstallTypeName_Freq\n",
      "Creating new feature: Census_OSBuildRevision_OsBuildLab_Freq\n",
      "Creating new feature: AppVersion_Census_OSInstallTypeName_Freq\n",
      "Creating new feature: AppVersion_OsBuildLab_Freq\n",
      "Creating new feature: Census_OSInstallTypeName_OsBuildLab_Freq\n"
     ]
    }
   ],
   "source": [
    "for keys in feature_groups:\n",
    "    new_col = '_'.join(keys) + '_Freq'\n",
    "    print('Creating new feature:', new_col)\n",
    "    cnt = data.groupby(keys).size().to_frame(new_col).reset_index()\n",
    "    data = pd.merge(data, cnt, how='left', on=keys, suffixes=('', '.cnt'))\n",
    "    data[new_col] /= len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'colsample_bytree': 0.28,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': -1,\n",
    "    'num_leaves': 2**12 - 1,\n",
    "    'n_estimators': 30000,\n",
    "    'objective': 'binary',\n",
    "    'random_state': seed,\n",
    "    'n_jobs': -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.606584\tvalid_0's auc: 0.730403\n",
      "[200]\tvalid_0's binary_logloss: 0.601514\tvalid_0's auc: 0.733196\n",
      "[300]\tvalid_0's binary_logloss: 0.601192\tvalid_0's auc: 0.733267\n",
      "Early stopping, best iteration is:\n",
      "[240]\tvalid_0's binary_logloss: 0.60101\tvalid_0's auc: 0.733561\n"
     ]
    }
   ],
   "source": [
    "_, preds = fit_single_split(\n",
    "    lgb.LGBMClassifier, params, trn_df, trn_target, tst_df, \n",
    "    eval_metric='auc',\n",
    "    verbose=100,\n",
    "    early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = submit(preds, DATA/'sample_submission.csv', 'count_feat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 391M/391M [00:39<00:00, 10.3MB/s]\n",
      "Successfully submitted to Microsoft Malware Prediction"
     ]
    }
   ],
   "source": [
    "!kaggle c submit -c microsoft-malware-prediction -f \"$out_file\" -m \"Kernel clone (+ cnt)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_trn, y_trn,\n",
    "          eval_metric='auc',\n",
    "          eval_set=[(x_val, y_val)],\n",
    "          verbose=100,\n",
    "          early_stopping_rounds=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
